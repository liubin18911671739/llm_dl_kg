你是一个人工智能专家。下面 <paper> </paper> tag之间的内容是一篇关于知识图谱的论文。请将这篇论文改写为一个可以通过重复审查的中文科技论文。

<paper>

# 《融合大语言模型与深度学习的知识图谱构建研究》

## 摘要

目的:本文提出了一种将大语言模型(如 ChatGPT、Claude 和 GPT-3)与深度学习相结合,用于构建高质量知识图谱的方法 LLM-DL-KG。该方法旨在利用大语言模型的优势来解决传统深度学习构建知识图谱面临的数据质量、多样性、模型复杂性、解释性、实体识别、关系抽取、知识一致性和时效性等问题。

方法:LLM-DL-KG 采用端到端的学习方式,利用大语言模型强大的语言理解和知识融合能力,从原始文本直接学习实体识别和关系抽取任务。同时,通过迁移学习降低模型训练复杂性,并利用大语言模型的持续学习能力维护知识图谱的一致性和时效性。我们在中英文数据集上进行了广泛实验,评估不同大语言模型在知识图谱构建任务上的表现,并与传统深度学习方法进行了比较。

结果:实验结果表明,将大语言模型融入知识图谱构建可以显著提高实体识别和关系抽取的准确性,尤其在低资源场景下优势明显。在多个数据集上,LLM-DL-KG 的 F1 值较优的基线模型平均提升 3~5 个百分点。同时,引入大语言模型也加速了训练和推理过程,平均耗时减少 20%以上。

局限:LLM-DL-KG 的效果在很大程度上依赖于底层大语言模型的性能,而当前这些模型在知识获取和更新方面仍存在一些局限性。此外,对于高度专业化的领域知识,大语言模型的知识增强效果有待进一步验证。

结论:LLM-DL-KG 为知识图谱构建任务提供了一种全新的思路,表明了大语言模型在知识获取和融合方面的巨大潜力。未来工作将致力于进一步提高大语言模型与知识图谱的融合程度,探索更高效的知识管理和更新机制,扩展方法在更多领域的应用。

## 1. 引言

知识图谱作为一种结构化知识库,在智能问答、推荐系统、语义搜索等领域发挥着重要作用[1]。传统知识图谱构建通常包括实体识别、关系抽取、知识融合等步骤,主要依赖规则或统计学习方法[2]。近年来,随着深度学习的发展,研究者们开始探索利用神经网络模型进行端到端的知识图谱构建[3,4]。然而,受限于标注数据的质量和规模,以及模型的复杂性和可解释性,深度学习在知识获取任务上的性能提升遇到了瓶颈[5]。

大语言模型的出现为解决上述问题带来了新的契机。GPT-3[6]、BERT[7]等预训练模型通过在大规模文本语料上进行自监督学习,习得了丰富的语言知识和常识,展现出强大的语义理解和知识泛化能力。最新的 ChatGPT[8]、Claude[9]等模型更是引入了持续学习机制,允许模型根据新信息动态更新知识库。大语言模型与知识图谱的结合正受到学术界和产业界的广泛关注。

本文提出了一种新颖的知识图谱构建方法 LLM-DL-KG,旨在充分发挥大语言模型在知识获取和融合方面的优势,同时利用深度学习模型的特征提取和泛化能力,实现高质量、高效率的知识图谱构建。具体而言,本文的贡献如下:
(1)提出将大语言模型与深度学习相结合进行知识图谱构建的端到端学习框架,有效解决了传统方法面临的数据质量、模型复杂性等问题;
(2)引入持续学习和知识融合机制,利用大语言模型的知识增强能力,提高知识图谱构建的准确性和时效性;
(3)在中英文数据集上进行了广泛实验,验证了 LLM-DL-KG 在实体识别、关系抽取等子任务上的有效性,并与多种基线模型进行了比较。

## 2. 相关研究

### 2.1 知识图谱构建

知识图谱构建旨在从非结构化文本中抽取实体、关系等知识元素,并组织为结构化的知识库。早期工作主要采用基于规则或统计的方法,如条件随机场(CRF)[10]、支持向量机(SVM)[11]等。这些方法需要大量人工特征工程,且泛化能力有限。近年来,研究者开始探索利用深度学习进行端到端的知识图谱构建。Zeng 等[12]提出了基于卷积神经网络(CNN)的 distant supervision 关系抽取模型。Lin 等[13]引入注意力机制,提高了实体-关系联合抽取的效果。针对低资源场景,Yang 等[14]提出了基于元学习的 few-shot 关系抽取方法。然而,现有深度学习方法在处理长尾实体和复杂语义时仍面临挑战。

### 2.2 大语言模型

大语言模型通过在大规模无标注语料上进行自监督预训练,习得了丰富的语言知识。早期的 word2vec[15] 和 GloVe[16] 等静态词嵌入模型为神经网络提供了高质量的文本表示。此后,ELMo[17]、GPT[18]等上下文动态表示模型进一步提高了语义建模能力。BERT[7]、RoBERTa[19]等双向语言模型在多项自然语言理解任务上取得了显著进展。GPT-3[6]更是展现了 few-shot 学习能力,无需微调即可适应下游任务。ChatGPT[8]、Claude[9]等最新对话模型引入了持续学习,允许根据反馈更新已学知识。大语言模型为知识获取和融合开辟了新的可能性。

### 2.3 知识增强

为解决标注数据匮乏问题,研究者提出利用外部知识增强神经网络模型。early fusion 策略在输入层引入知识特征,如将知识库中实体对应的 TransE 嵌入与词嵌入拼接[20]。late fusion 策略在输出层集成外部知识,如用知识库规则约束解码[21]。此外,知识蒸馏[22]和对比学习[23]也被用于将结构化知识整合至神经网络权重。但这些方法主要利用人工构建的知识库,受限于知识覆盖范围。本文提出利用大语言模型的隐式知识进行增强,无需人工干预,具有更好的灵活性和扩展性。

## 3. 方法

### 3.1 LLM-DL-KG 总体框架

LLM-DL-KG 采用端到端的学习范式,以大语言模型作为骨干网络,通过持续学习和迁移学习机制,实现从原始文本到知识图谱的直接映射。图 1 展示了整体框架,主要包括 4 个模块:

(1) 知识表示模块:借助预训练语言模型学习文本的分布式表示,捕捉丰富的语义和知识特征;

(2) 实体识别模块:在语言模型输出的动态嵌入之上,通过条件随机场等序列标注模型识别出句子中的候选实体;

(3) 关系抽取模块:将实体识别结果作为输入,利用深度神经网络学习实体对之间的语义关系;

(4) 知识融合模块:综合多源信息解决知识跨文本对齐,并利用大语言模型的持续学习能力动态更新知识库。

图 1 LLM-DL-KG 总体框架

### 3.2 知识表示模块

给定输入句子 X={x1,…,xn},知识表示模块旨在学习其上下文相关的分布式表征。传统的 word2vec 等静态词嵌入难以刻画复杂语义。因此,本文采用预训练语言模型作为骨干网络,以其最后一层的输出作为动态词嵌入。具体地,本文使用 RoBERTa[19]作为基础模型,其优势在于采用动态 masking 和更大的 batch size,有助于学习更鲁棒的上下文表示。句子经过 tokenize、mask 等预处理后,输入 RoBERTa 编码器:

$\mathbf{H} = \mathrm{RoBERTa}(\mathbf{X})$

其中 H ∈ Rn × dh 为句子的动态嵌入表示,dh 为隐藏层维度。

考虑到大语言模型已经习得了丰富的知识,我们利用 prompt learning 范式,引入少量模板信息辅助特定任务学习。以实体识别为例,设计如下形式的 prompt:

X′=[CLS] X [SEP] Find all entities in the text. [SEP]
将 prompt 文本与原始输入拼接编码,得到增强表示:

$\mathbf{H}^\prime = \mathrm{RoBERTa}(\mathbf{X}^\prime)$

这种 prompt 方式可以激活语言模型中与实体相关的知识,辅助下游序列标注任务。实验表明,引入 prompt 后,F1 值平均提升 1~2 个百分点。

### 3.3 实体识别模块

实体识别旨在从输入句子中抽取出关键实体。本文在 RoBERTa 的动态嵌入之上,接入条件随机场(CRF)层,进行序列标注:

$P(\mathbf{Y}|\mathbf{X}) = \mathrm{CRF}(\mathbf{H}^\prime)$

其中 Y={y1,…,yn}为实体标签序列。相比 Softmax,CRF 能够考虑相邻标签之间的约束,更适用于序列标注任务。在训练阶段,采用负对数似然 loss:

$\mathrm{loss} = -\log P(\mathbf{Y}|\mathbf{X}) = -\log \frac{\sum\exp(s(\mathbf{X},\mathbf{Y}))}{Z(\mathbf{X})}$

其中 s(X,Y)为发射分和转移分之和,Z(X)为归一化因子。在推理阶段,采用 Viterbi 解码寻找得分最高的标签序列。对于嵌套实体,本文采用 Pyramid-CRF 框架[24],通过迭代合并不同粒度的片段,逐层识别出嵌套实体。

### 3.4 关系抽取模块

关系抽取旨在判断实体对之间是否存在特定语义关系。给定句子 X 及其中的两个目标实体 eh 和 et,传统方法通常采用 pipeline 形式,先识别实体,再抽取关系。这忽略了两个子任务之间的依赖关系。为了实现实体-关系联合抽取,本文设计了基于指针网络[25]的端到端模型。
具体地,将目标实体对的位置信息编码为向量 qh 和 qt,与实体嵌入拼接后输入分类器:

$P(r|\mathbf{X},e_h,e_t) = \sigma(\mathrm{MLP}([\mathbf{H}^\prime_h;\mathbf{q}_h;\mathbf{H}^\prime_t;\mathbf{q}_t]))$

其中[;]表示拼接操作,MLP 为多层感知机。为了同时考虑句子级上下文信息和实体对局部特征,我们进一步引入注意力机制[26]:

$\alpha_i = \mathrm{softmax}((\mathbf{H}^\prime_i\mathbf{W}\_Q)(\mathbf{H}^\prime_t\mathbf{W}\_K)^\top)$

$\mathbf{c} = \sum_{i}\alpha_i(\mathbf{H}^\prime_i\mathbf{W}_V)$

其中 WQ,WK,WV 为注意力参数矩阵。将注意力向量 c 与实体表示拼接,得到关系分类的最终特征。
训练时,采用二元交叉熵 loss:

$\mathrm{loss} = -\sum_{(X,e_h,e_t,r)\in\mathcal{D}}(r\log(P(r=1|\mathbf{X},e_h,e_t))+(1-r)\log(1-P(r=1|\mathbf{X},e_h,e_t)))$

推理时,对每个候选实体对计算关系概率,超过阈值则认为存在对应关系。
此外,本文还探索了利用大语言模型进行关系抽取的 few-shot 学习范式。具体地,将少量标注样本构造为 prompt 形式:

X′=[CLS] X [SEP] The relation between entity1 and entity2 is [MASK]. [SEP]

预测[MASK]位置的概率分布,从而判断实体对的关系类型:

$P(r|\mathbf{X},e_h,e_t) = \mathrm{Softmax}(\mathrm{MLP}(\mathbf{H}^\prime_{\mathrm{[MASK]}}))$

其中 H′[MASK]为[MASK]令牌对应的嵌入向量。这种 few-shot 方式可以充分利用大语言模型学到的背景知识,在小样本场景下取得较好效果。

### 3.5 知识融合模块

前述模块从单个文档抽取局部知识,知识融合模块进一步考虑信息的全局一致性,构建文档间的关联。传统工作通过人工定义规则或相似度度量进行实体对齐。本文提出利用大语言模型进行跨文本的知识推理。
具体地,给定两个文档 d1 和 d2 及其抽取的局部知识图谱 g1 和 g2,我们将其拼接为自然语言形式的 prompt:

X′=[CLS] g1 [SEP] g2 [SEP] Are the entity eh in g1 and et in g2 referring to the same object? [SEP]

喂入预训练语言模型,根据[CLS]表示进行二分类:

$P(e_h \equiv e_t|\mathbf{g}_1,\mathbf{g}_2) = \mathrm{Sigmoid}(\mathrm{MLP}(\mathbf{H}^\prime{\mathrm{[CLS]}}))$

其中 H′[CLS] 为 [CLS] 令牌对应的嵌入向量。

利用大语言模型强大的语义理解和推理能力,可以挖掘出不同文档间的隐含联系,实现跨文本的知识融合。
此外,本文还引入知识蒸馏机制,利用教师模型(大语言模型)指导学生模型(知识融合模块)学习:

$\mathrm{loss} = D_{\mathrm{KL}}(P_T(\cdot)||P_S(\cdot|\theta))$

其中 θ 为学生模型参数,DKL 为 KL 散度。通过这种方式,可以将大语言模型中的结构化知识迁移到下游任务中。

最后,为了维护知识库的动态更新,本文设计了基于增量学习[27]的持续学习机制。当新的实体或关系加入时,通过优化梯度影响函数(GEM)来选择性更新模型参数,避免灾难性遗忘:

$\min_{\theta^\prime} L(\theta^\prime) \quad \mathrm{s.t.} \quad L(\theta^\prime) - L(\theta) \geq 0$

其中 θ′为更新后的参数,L 为损失函数。结合少样本学习和知识蒸馏,LLM-DL-KG 可以高效地融合新增知识,构建实时更新的知识图谱。

## 4. 实验

### 4.1 实验数据集

为了验证 LLM-DL-KG 在不同语言和领域的适用性,本文选取了 5 个中英文知识图谱数据集:

(1)CN-DBpedia[28]:中文百科知识图谱,包含 1000 万实体和 6000 万关系;

(2)OwnThink[29]:多源融合的中文知识图谱,包含 1.4 亿实体和 2.4 亿关系;

(3)OpenKG[30]:融合百度百科、互动百科等的开放中文知识图谱;

(4)NYT[31]:纽约时报英文新闻语料,包含实体、关系和事件标注;

(5)PubMed[32]:生物医学文献知识图谱,包含疾病、药物、基因等实体及关系。

各数据集的统计信息如表 1 所示:

| 数据集     | 语种 | 领域     | 实体数  | 关系数  |
| ---------- | ---- | -------- | ------- | ------- |
| CN-DBpedia | 中   | 百科知识 | 1000 万 | 6000 万 |
| OwnThink   | 中   | 百科知识 | 1.4 亿  | 2.4 亿  |
| OpenKG     | 中   | 百科知识 | -       | 43 亿   |
| NYT        | 英   | 新闻     | 217,132 | 67,537  |
| PubMed     | 英   | 生物医学 | 134 万  | 250 万  |

表 1 实验数据集统计

我们采用 8:1:1 的比例随机划分训练集、验证集和测试集。为缓解标注数据规模有限的问题,在中文数据集上采用 5 折交叉验证,英文数据集采用 distant supervision 方法自动构建训练语料。

4.2 实验设置

LLM-DL-KG 模型的骨干网络为 RoBERTa-wwm-ext-large[33],最大序列长度设为 512。Adam 优化器学习率为 1e-5,batch size 为 64。其他超参数通过网格搜索确定。实验在配备 A100 GPU 的服务器上进行,框架为 PyTorch 和 Huggingface Transformers。知识库存储和查询采用图数据库 Neo4j。

对比方法包括:

(1)BiLSTM-CRF[34]:基于双向 LSTM 和 CRF 的经典序列标注模型;

(2)BERT-tagger[35]:在 BERT 之上搭建 CRF 层的实体识别模型;

(3)BERT-Matching[36]:将关系抽取视为句子级多分类任务的 BERT 模型;

(4)GPT-3[6]:基于 few-shot 学习的大规模语言模型;

(5)WDec[37]:融合词汇依赖的实体-关系联合抽取模型;

(6)PFN[38]:基于指针网络的端到端关系抽取模型。

评价指标采用准确率(Precision)、召回率(Recall)和 F1 值。实体识别采用严格匹配,关系抽取采用部分匹配。结果取 5 次运行的平均值。

4.3 实验结果
表 2 展示了 LLM-DL-KG 与对比方法在 5 个数据集上的实体识别效果:

| 模型\数据集 | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ----------- | ---------- | -------- | ------ | ----- | ------ |
| BiLSTM-CRF  | 83.21      | 80.56    | 80.12  | 85.09 | 78.63  |
| BERT-tagger | 87.54      | 84.83    | 84.35  | 88.26 | 81.92  |
| GPT-3(few)  | 84.16      | -        | -      | 86.31 | 80.54  |
| LLM-DL-KG   | 89.72      | 86.59    | 86.02  | 90.37 | 84.25  |

表 2 实体识别结果(F1 值)

可以看出,LLM-DL-KG 在各数据集上均取得了最佳表现,平均提升 3~5 个百分点。相比 BiLSTM 等传统方法,基于预训练语言模型的方法效果显著。进一步引入持续学习和知识融合策略后,LLM-DL-KG 的优势更加明显,尤其在低资源场景下领先 GPT-3 等 few-shot 方法 3~4 个点,表明大语言模型与深度学习的结合可显著提升实体识别能力。

表 3 展示了 LLM-DL-KG 与对比方法在 5 个数据集上的关系抽取效果:

| 模型\数据集   | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ------------- | ---------- | -------- | ------ | ----- | ------ |
| BERT-Matching | 60.25      | 55.17    | 52.03  | 62.14 | 57.36  |
| WDec          | 62.08      | 57.92    | 54.35  | 63.87 | 59.21  |
| PFN           | 64.31      | 59.68    | 56.29  | 65.56 | 60.78  |
| GPT-3(few)    | 65.47      | -        | -      | 66.23 | 61.95  |
| LLM-DL-KG     | 68.24      | 63.35    | 59.82  | 69.71 | 64.59  |

表 3 关系抽取结果(F1 值)

可以看出,采用端到端学习范式的模型(PFN、LLM-DL-KG)整体优于 pipeline 方法,证实了实体-关系联合抽取的有效性。同时,引入注意力机制和 few-shot 学习策略,LLM-DL-KG 在多个数据集上取得最佳效果,较优的基线模型平均提升 3~5 个百分点。此外,基于大语言模型的持续学习,LLM-DL-KG 可有效融合增量数据,在知识更新较频繁的 PubMed 数据集上提升尤为明显。

图 2 展示了 LLM-DL-KG 与传统深度学习方法(BERT-CRF)在训练效率上的比较:

图 2 LLM-DL-KG 与 BERT-CRF 的训练耗时对比

可以看出,引入大语言模型和迁移学习策略后,LLM-DL-KG 的收敛速度明显加快,训练耗时平均减少 20%以上。这主要得益于大语言模型习得的知识可显著降低模型训练复杂度。同时,持续学习机制也提高了模型适应新增数据的效率。

为验证 LLM-DL-KG 在知识融合方面的能力,我们统计了不同数据集上知识库的平均查全率,如表 4 所示:

| 模型\数据集 | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ----------- | ---------- | -------- | ------ | ----- | ------ |
| Pipeline    | 72.15      | 68.32    | 64.59  | 75.88 | 70.23  |
| LLM-DL-KG   | 84.96      | 81.37    | 77.82  | 88.11 | 83.45  |

表 4 不同知识融合策略的查全率(%)
可以看出,pipeline 方式由于缺乏文档间知识关联,在查全率上普遍较低。LLM-DL-KG 利用大语言模型进行跨文本知识推理,显著提升了知识库的覆盖度,平均查全率达 85%以上。同时,增量学习策略可有效兼顾知识融合的效率和一致性。
综上所述,实验结果表明,LLM-DL-KG 在多个数据集上取得了优异的知识图谱构建效果。将大语言模型引入知识获取和融合环节,可显著改善实体识别、关系抽取的准确性,加快模型训练速度,并提高知识库的覆盖度。

## 5.结语

本文提出了一种融合大语言模型与深度学习的知识图谱构建方法 LLM-DL-KG。针对现有方法面临的数据质量、模型复杂性等问题,LLM-DL-KG 利用预训练语言模型的知识增强能力和持续学习机制,实现高效、准确、全面的知识图谱构建。在中英文数据集上的实验表明,LLM-DL-KG 可显著提升实体识别、关系抽取的效果,加快训练速度,并改善知识库的时效性和完备性。

未来工作主要包括:(1)进一步提高大语言模型与下游任务的适配性,如引入对比学习、因果发现等策略;(2)拓展方法在更多垂直领域知识图谱构建中的应用,如金融、医疗、法律等;(3)探索大语言模型在知识推理、问答等下游任务中的增强机制,实现知识的端到端应用。

参考文献:

[1] Wang Q, Mao Z, Wang B, et al. Knowledge graph embedding: A survey of approaches and applications[J]. TKDE, 2017.

[2] Ji S, Pan S, Cambria E, et al. A survey on knowledge graphs: Representation, acquisition, and applications[J]. TNNLS, 2021.

[3] Qian Y, Santus E, Jin Z, et al. A survey on deep learning for named entity recognition[J]. TKDE, 2021.

[4] Zhang Z, Han X, Liu Z, et al. ERNIE: Enhanced language representation with informative entities[C]. ACL, 2019.

[5] Chen H, Liu X, Yin D, et al. A survey on dialogue systems: Recent advances and new frontiers[J]. SIGKDD Explorations, 2017.

[6] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. NeurIPS, 2020.

[7] Devlin J, Chang M, Lee K, et al. BERT: Pre-training of deep bidirectional transformers for language understanding[C]. NAACL, 2019.

[8] Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. arXiv:2203.02155, 2022.

[9] Claude AI. https://www.anthropic.com

[10] Lafferty J, McCallum A, Pereira F. Conditional random fields: Probabilistic models for segmenting and labeling sequence data[C]. ICML, 2001.

[11] Cortes C, Vapnik V. Support-vector networks[J]. Machine learning, 1995.

[12] Zeng D, Liu K, Chen Y, et al. Distant supervision for relation extraction via piecewise convolutional neural networks[C]. EMNLP, 2015.

[13] Lin Y, Shen S, Liu Z, et al. Neural relation extraction with selective attention over instances[C]. ACL, 2016.

[14] Yang F, Zou Q. FewRel 2.0: Towards more challenging few-shot relation extraction[C]. EMNLP, 2020.

[15] Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. NeurIPS, 2013.

[16] Pennington J, Socher R, Manning C. GloVe: Global vectors for word representation[C]. EMNLP, 2014.

[17] Peters M, Neumann M, Iyyer M, et al. Deep contextualized word representations[C]. NAACL, 2018.

[18] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. OpenAI Blog, 2018.

[19] Liu Y, Ott M, Goyal N, et al. RoBERTa: A robustly optimized BERT pretraining approach[J]. arXiv:1907.11692, 2019.

[20] Zhong Z, Chen D. A frustratingly easy approach for joint entity and relation extraction[C]. NAACL, 2021.

[21] Zhang Y, Liu K, He S, et al. An interactive approach to enhancing the alignment between a knowledge graph and a knowledge base[C]. WWW, 2022.

[22] Zhang Z, Li H, Yao J, et al. Knowledge distillation for few-shot named entity recognition[J]. ACL, 2022.

[23] Niu Y, Zhong H. Contrastive learning of structured world models[J]. CVPR, 2022.

[24] Li C, Qiu X, Chen W, et al. Pyramid: A nested model for hierarchical entity recognition[C]. AAAI, 2022.

[25] Vinyals O, Fortunato M, Jaitly N. Pointer networks[J]. NeurIPS, 2015.

[26] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. NeurIPS, 2017.

[27] Castro F, Marín-Jiménez M, Guil N, et al. End-to-end incremental learning[C]. ECCV, 2018.

[28] Xu B, Xu Y, Liang J, et al. CN-DBpedia: A never-ending Chinese knowledge extraction system[C]. AAAI, 2017.

[29] OwnThink. https://www.ownthink.com

[30] OpenKG. http://www.openkg.cn

[31] Riedel S, Yao L, McCallum A. Modeling relations and their mentions without labeled text[C]. ECML-PKDD, 2010.

[32] Wei C, Peng Y, Leaman R, et al. Overview of the BioCreative V chemical disease relation (CDR) task[C]. BioCreative, 2015.

[33] Cui Y, Che W, Liu T, et al. Pre-training with whole word masking for Chinese BERT[J]. TASLP, 2021.

[34] Chen Z, Liang Y, Pan X, et al. Effective combination of DNN and CRF for Chinese named entity recognition[J]. TASLP, 2021.

[35] Sui D, Chen Y, Liu K, et al. Leverage lexical knowledge for Chinese named entity recognition via collaborative graph network[C]. EMNLP, 2019.

[36] Gao T, Han X, Liu Z, et al. Hybrid attention-based prototypical networks for noisy few-shot relation classification[C]. AAAI, 2019.

[37] Zhang D, He J, Hu Q. Word-level dependency guided neural relation extraction[C]. EMNLP, 2021.

[38] Gao T, Huang J, Wang L, et al. Pointer-free joint extraction of entities and relations[J]. TASLP, 2022.

</paper>