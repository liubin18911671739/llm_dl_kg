
Please response as a Skilled SEO Content Creator with impeccable English proficiency. Initiate with Two Tables. The initial table will represent the article's framework, while the subsequent one will embody the content itself. Bold the title of the second table employing Markdown syntax. Begin by drafting a detailed outline for the article, incorporating at least 15 main and sub-points (spanning H1, H2, H3, and H4 tags). Progressively compose based on this outline. Produce a 8000-word wholly original, SEO-enhanced, human-crafted composition in English with a minimum of 15 titles and subtitles (encompassing H1, H2, H3, and H4 labels) pertinent to the topic described in the Prompt. Ensure the piece is genuine, refraining from direct lifts from other references. While formulating, prioritize complexity and spontaneity, achieving optimal levels of both without sacrificing relevance or backdrop. Frame rich, engaging paragraphs. Adopt a colloquial tone, as if penned by a human (Maintain Simplicity, Involve the Audience, Employ an Active Voice, Stay Concise, Pose Rhetorical Inquiries, and Integrate Comparisons and Imagery). Conclude with a summarizing segment and a set of 5 distinct FAQs post this summary. It's crucial to emphasize the main title and all sub-titles of the composition, ensuring proper hierarchical tag usage. Now, please draft an essay on the subsequent subject: 



### 4.1 实验数据集

为了验证 LLM-DL-KG 在不同语言和领域的适用性,本文选取了 5 个中英文知识图谱数据集:

(1)CN-DBpedia[28]:中文百科知识图谱,包含 1000 万实体和 6000 万关系;

(2)OwnThink[29]:多源融合的中文知识图谱,包含 1.4 亿实体和 2.4 亿关系;

(3)OpenKG[30]:融合百度百科、互动百科等的开放中文知识图谱;

(4)NYT[31]:纽约时报英文新闻语料,包含实体、关系和事件标注;

(5)PubMed[32]:生物医学文献知识图谱,包含疾病、药物、基因等实体及关系。

各数据集的统计信息如表 1 所示:

| 数据集     | 语种 | 领域     | 实体数  | 关系数  |
| ---------- | ---- | -------- | ------- | ------- |
| CN-DBpedia | 中   | 百科知识 | 1000 万 | 6000 万 |
| OwnThink   | 中   | 百科知识 | 1.4 亿  | 2.4 亿  |
| OpenKG     | 中   | 百科知识 | -       | 43 亿   |
| NYT        | 英   | 新闻     | 217,132 | 67,537  |
| PubMed     | 英   | 生物医学 | 134 万  | 250 万  |

表 1 实验数据集统计

我们采用 8:1:1 的比例随机划分训练集、验证集和测试集。为缓解标注数据规模有限的问题,在中文数据集上采用 5 折交叉验证,英文数据集采用 distant supervision 方法自动构建训练语料。

4.2 实验设置

LLM-DL-KG 模型的骨干网络为 RoBERTa-wwm-ext-large[33],最大序列长度设为 512。Adam 优化器学习率为 1e-5,batch size 为 64。其他超参数通过网格搜索确定。实验在配备 A100 GPU 的服务器上进行,框架为 PyTorch 和 Huggingface Transformers。知识库存储和查询采用图数据库 Neo4j。

对比方法包括:

(1)BiLSTM-CRF[34]:基于双向 LSTM 和 CRF 的经典序列标注模型;

(2)BERT-tagger[35]:在 BERT 之上搭建 CRF 层的实体识别模型;

(3)BERT-Matching[36]:将关系抽取视为句子级多分类任务的 BERT 模型;

(4)GPT-3[6]:基于 few-shot 学习的大规模语言模型;

(5)WDec[37]:融合词汇依赖的实体-关系联合抽取模型;

(6)PFN[38]:基于指针网络的端到端关系抽取模型。

评价指标采用准确率(Precision)、召回率(Recall)和 F1 值。实体识别采用严格匹配,关系抽取采用部分匹配。结果取 5 次运行的平均值。

4.3 实验结果
表 2 展示了 LLM-DL-KG 与对比方法在 5 个数据集上的实体识别效果:

| 模型\数据集 | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ----------- | ---------- | -------- | ------ | ----- | ------ |
| BiLSTM-CRF  | 83.21      | 80.56    | 80.12  | 85.09 | 78.63  |
| BERT-tagger | 87.54      | 84.83    | 84.35  | 88.26 | 81.92  |
| GPT-3(few)  | 84.16      | -        | -      | 86.31 | 80.54  |
| LLM-DL-KG   | 89.72      | 86.59    | 86.02  | 90.37 | 84.25  |

表 2 实体识别结果(F1 值)

可以看出,LLM-DL-KG 在各数据集上均取得了最佳表现,平均提升 3~5 个百分点。相比 BiLSTM 等传统方法,基于预训练语言模型的方法效果显著。进一步引入持续学习和知识融合策略后,LLM-DL-KG 的优势更加明显,尤其在低资源场景下领先 GPT-3 等 few-shot 方法 3~4 个点,表明大语言模型与深度学习的结合可显著提升实体识别能力。

表 3 展示了 LLM-DL-KG 与对比方法在 5 个数据集上的关系抽取效果:

| 模型\数据集   | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ------------- | ---------- | -------- | ------ | ----- | ------ |
| BERT-Matching | 60.25      | 55.17    | 52.03  | 62.14 | 57.36  |
| WDec          | 62.08      | 57.92    | 54.35  | 63.87 | 59.21  |
| PFN           | 64.31      | 59.68    | 56.29  | 65.56 | 60.78  |
| GPT-3(few)    | 65.47      | -        | -      | 66.23 | 61.95  |
| LLM-DL-KG     | 68.24      | 63.35    | 59.82  | 69.71 | 64.59  |

表 3 关系抽取结果(F1 值)

可以看出,采用端到端学习范式的模型(PFN、LLM-DL-KG)整体优于 pipeline 方法,证实了实体-关系联合抽取的有效性。同时,引入注意力机制和 few-shot 学习策略,LLM-DL-KG 在多个数据集上取得最佳效果,较优的基线模型平均提升 3~5 个百分点。此外,基于大语言模型的持续学习,LLM-DL-KG 可有效融合增量数据,在知识更新较频繁的 PubMed 数据集上提升尤为明显。

图 2 展示了 LLM-DL-KG 与传统深度学习方法(BERT-CRF)在训练效率上的比较:

图 2 LLM-DL-KG 与 BERT-CRF 的训练耗时对比

可以看出,引入大语言模型和迁移学习策略后,LLM-DL-KG 的收敛速度明显加快,训练耗时平均减少 20%以上。这主要得益于大语言模型习得的知识可显著降低模型训练复杂度。同时,持续学习机制也提高了模型适应新增数据的效率。

为验证 LLM-DL-KG 在知识融合方面的能力,我们统计了不同数据集上知识库的平均查全率,如表 4 所示:

| 模型\数据集 | CN-DBpedia | OwnThink | OpenKG | NYT   | PubMed |
| ----------- | ---------- | -------- | ------ | ----- | ------ |
| Pipeline    | 72.15      | 68.32    | 64.59  | 75.88 | 70.23  |
| LLM-DL-KG   | 84.96      | 81.37    | 77.82  | 88.11 | 83.45  |

表 4 不同知识融合策略的查全率(%)
可以看出,pipeline 方式由于缺乏文档间知识关联,在查全率上普遍较低。LLM-DL-KG 利用大语言模型进行跨文本知识推理,显著提升了知识库的覆盖度,平均查全率达 85%以上。同时,增量学习策略可有效兼顾知识融合的效率和一致性。
综上所述,实验结果表明,LLM-DL-KG 在多个数据集上取得了优异的知识图谱构建效果。将大语言模型引入知识获取和融合环节,可显著改善实体识别、关系抽取的准确性,加快模型训练速度,并提高知识库的覆盖度。