# 知识表示模块

**知识表示模块** 是 LLM-DL-KG 框架中的重要组成部分,主要目的是学习输入文本的低维、稠密的分布式表示,为后续的实体识别、关系抽取等任务提供高质量的特征。该模块的核心是预训练的语言模型,如 BERT、RoBERTa 等。

**输入**:

- 原始文本序列 $\mathbf{X}=\{x_1,\ldots,x_n\}$

**输出**:

- 文本的上下文表示 $\mathbf{H}=\{\mathbf{h}_1,\ldots,\mathbf{h}_n\}$

**主要步骤**:

1. **文本预处理**:
   - 对输入文本进行 tokenization、序列截断、填充等操作,转换为模型可接受的格式
   - 加入特殊标记,如`[CLS]`、`[SEP]`等,用于指示句子的开始和结束位置
2. **词嵌入**:
   - 将每个 token 映射为 dense vector $\mathbf{e}_i \in \mathbb{R}^{d_e}$
   - 词嵌入可以随机初始化,也可以使用预训练的词向量如 GloVe、word2vec 等
3. **位置嵌入**:
   - 为每个 token 的位置索引 $i$ 分配可学习的位置向量 $\mathbf{p}_i \in \mathbb{R}^{d_p}$
   - 位置嵌入能够帮助模型捕捉词序信息
4. **Segment 嵌入(可选)**:
   - 对于包含多个句子的输入,可以引入额外的 segment 向量 $\mathbf{s}_i \in \mathbb{R}^{d_s}$ 区分不同的句子
   - BERT 使用 segment 嵌入处理句对分类任务
5. **三种嵌入相加**:
   - 将词嵌入、位置嵌入和 segment 嵌入(如果有)按位相加,得到每个 token 的输入表示
   - $\mathbf{h}_i^{(0)} = \mathbf{e}_i + \mathbf{p}_i + \mathbf{s}_i$
6. **预训练语言模型编码**:
   - 将嵌入序列 $\mathbf{H}^{(0)}=\{\mathbf{h}_1^{(0)},\ldots,\mathbf{h}_n^{(0)}\}$ 输入预训练语言模型
   - 语言模型通常由多层 Transformer 编码器组成,可以建模长距离依赖关系
   - 公式表示为: $\mathbf{H}^{(l)} = \mathrm{Transformer}(\mathbf{H}^{(l-1)}), l=1,\ldots,L$
   - 最后一层的输出 $\mathbf{H}^{(L)}$ 即为文本的上下文表示
7. **融入 prompt(可选)**:
   - 在输入序列中插入与任务相关的提示模板,引导语言模型关注特定信息
   - prompt 可以是自然语言形式的问题或描述,也可以是人工设计的离散符号
   - 将 prompt 插入输入序列: $\mathbf{X}^\prime = \{\mathrm{[CLS]}, \mathbf{x}_\mathrm{prompt}, \mathrm{[SEP]}, x_1, \ldots, x_n, \mathrm{[SEP]}\}$
   - 语言模型在 prompt 上的输出可以用于指导下游任务

知识表示模块充分利用了预训练语言模型在大规模无监督语料上学习到的丰富语义和语法知识,为后续的实体识别、关系抽取等任务提供了高质量的特征表示。同时,通过引入 prompt,可以进一步提升语言模型在特定任务上的表现。知识表示模块的有效性直接影响到整个 LLM-DL-KG 框架的性能。
